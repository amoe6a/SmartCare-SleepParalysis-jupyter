{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path = \"/workspaces/codespaces-jupyter/data/YAAD/\"\n",
    "stimulus_descripttion_file = pd.read_excel(path + 'Stimulus_Description.xlsx')\n",
    "\n",
    "fear_video_id = stimulus_descripttion_file.query('`Target Emotion` == \"fear\"').head(1).get('Video ID').item()\n",
    "\n",
    "path_single_modal_data = path + \"RawData/Singlemodal/ECG/\"\n",
    "path_multi_modal_data = path + \"RawData/Multimodal/ECG/\"\n",
    "\n",
    "single_modal_data_files = os.listdir(path_single_modal_data)\n",
    "multi_modal_data_files = os.listdir(path_multi_modal_data)\n",
    "\n",
    "single_modal_dfs = []\n",
    "for file in single_modal_data_files:\n",
    "    if \"v\" + str(fear_video_id) in file:\n",
    "        df = pd.read_csv(path_single_modal_data + file, delimiter='\\t')\n",
    "        single_modal_dfs.append(df)\n",
    "\n",
    "multi_modal_dfs = []\n",
    "for file in multi_modal_data_files:\n",
    "    if \"v\" + str(fear_video_id) in file:\n",
    "        df = pd.read_csv(path_multi_modal_data + file, delimiter='\\t')\n",
    "        multi_modal_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_dfs_to_real_dfs(dfs) -> pd.array:\n",
    "    real_dfs = []\n",
    "    for df in dfs:\n",
    "        arr = df.columns.values\n",
    "        str_vals = arr[0].split(',')\n",
    "        real_vals = [float(each) for each in str_vals]\n",
    "        real_vals_pd = pd.array(real_vals)\n",
    "        real_dfs.append(real_vals_pd)\n",
    "    return pd.array(real_dfs)\n",
    "\n",
    "def mean_of_means(dfs) -> float:\n",
    "    t = []\n",
    "    for each in dfs:\n",
    "        t.append(each.mean())\n",
    "    return pd.array(t).mean()\n",
    "\n",
    "def min_of_mins(dfs) -> float:\n",
    "    t = []\n",
    "    for each in dfs:\n",
    "        t.append(each.min())\n",
    "    return pd.array(t).min()\n",
    "\n",
    "def max_of_maxs(dfs) -> float:\n",
    "    t = []\n",
    "    for each in dfs:\n",
    "        t.append(each.max())\n",
    "    return pd.array(t).max()\n",
    "\n",
    "def mean_of_vars(dfs) -> float:\n",
    "    t = []\n",
    "    for each in dfs:\n",
    "        t.append(each.var())\n",
    "    return pd.array(t).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_sm_dfs = str_dfs_to_real_dfs(single_modal_dfs)\n",
    "real_mm_dfs = str_dfs_to_real_dfs(multi_modal_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single refers to singlemodal data\n",
    "multi refers to multimodal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of means single:  -18.522971550372002\n",
      "mean of means multi:  -10.029048553400726\n",
      "min of mins single:  -297.21\n",
      "min of mins multi:  -21.106\n",
      "max of maxs single:  18.678\n",
      "max of maxs multi:  8.1532\n",
      "mean of vars single:  0.13076550723324526\n",
      "mean of vars multi:  0.041651835869248255\n"
     ]
    }
   ],
   "source": [
    "print(\"mean of means single: \", mean_of_means(real_sm_dfs))\n",
    "print(\"mean of means multi: \", mean_of_means(real_mm_dfs))\n",
    "\n",
    "print(\"min of mins single: \", min_of_mins(real_sm_dfs))\n",
    "print(\"min of mins multi: \", min_of_mins(real_mm_dfs))\n",
    "\n",
    "print(\"max of maxs single: \", max_of_maxs(real_sm_dfs))\n",
    "print(\"max of maxs multi: \", max_of_maxs(real_mm_dfs))\n",
    "\n",
    "print(\"mean of vars single: \", mean_of_vars(real_sm_dfs))\n",
    "print(\"mean of vars multi: \", mean_of_vars(real_mm_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_modal_df = []\n",
    "columns = list(range(1, 5001)) + [\"Vtype\"]\n",
    "\n",
    "# We put vtype 1 for fear sessions, and vtype 0 for everything else\n",
    "for file in single_modal_data_files:\n",
    "    df = pd.read_csv(path_single_modal_data + file, delimiter='\\t')\n",
    "    vtype = float(file.split('v')[1][0])\n",
    "\n",
    "    vtype = 1 if vtype == 7.0 else 0\n",
    "\n",
    "    rdf = str_dfs_to_real_dfs([df])[0]\n",
    "    minval = rdf.min()\n",
    "    rdf_and_vtype = pd.array([(each - minval) for each in rdf] + [vtype])\n",
    "    single_modal_df.append(rdf_and_vtype)\n",
    "single_modal_df = pd.DataFrame(single_modal_df, columns=columns)\n",
    "\n",
    "\n",
    "multi_modal_df = []\n",
    "for file in multi_modal_data_files:\n",
    "    df = pd.read_csv(path_multi_modal_data + file, delimiter='\\t')\n",
    "    vtype = float(file.split('v')[1][0])\n",
    "\n",
    "    vtype = 1 if vtype == 7.0 else 0\n",
    "\n",
    "    rdf = str_dfs_to_real_dfs([df])[0]\n",
    "    minval = rdf.min()\n",
    "    rdf_and_vtype = pd.array([(each - minval) for each in rdf] + [vtype])\n",
    "    multi_modal_df.append(rdf_and_vtype)\n",
    "multi_modal_df = pd.DataFrame(multi_modal_df, columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_y = single_modal_df.Vtype\n",
    "single_X = single_modal_df[list(range(1, 5001))]\n",
    "\n",
    "multi_y = multi_modal_df.Vtype\n",
    "multi_X = multi_modal_df[list(range(1, 5001))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we train using single modal data and validate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11507936507936507\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "fear_detect_model = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "fear_detect_model.fit(single_X, single_y)\n",
    "\n",
    "val_predictions = fear_detect_model.predict(multi_X)\n",
    "print(mean_absolute_error(multi_y, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train using multi modal data (inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12337662337662338\n"
     ]
    }
   ],
   "source": [
    "fear_detect_model.fit(multi_X, multi_y)\n",
    "\n",
    "val_predictions = fear_detect_model.predict(single_X)\n",
    "print(mean_absolute_error(single_y, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training for single shows a bit less mean error\n",
    "Now, we will try to find optimal max_leaf_nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_leaves(max_leaf_nodes, train_X, train_y, val_X, val_y):\n",
    "    fear_detect_model = DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes, random_state=1)\n",
    "    fear_detect_model.fit(train_X, train_y)\n",
    "\n",
    "    val_predictions = fear_detect_model.predict(val_X)\n",
    "    return mean_absolute_error(val_y, val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_leaf_nodes = 10, train-single mae:  0.11507936507936507\n",
      "max_leaf_nodes = 10, train-multi mae:  0.12987012987012986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_leaf_nodes = 50, train-single mae:  0.11507936507936507\n",
      "max_leaf_nodes = 50, train-multi mae:  0.12337662337662338\n",
      "max_leaf_nodes = 100, train-single mae:  0.11507936507936507\n",
      "max_leaf_nodes = 100, train-multi mae:  0.12337662337662338\n",
      "max_leaf_nodes = 500, train-single mae:  0.11507936507936507\n",
      "max_leaf_nodes = 500, train-multi mae:  0.12337662337662338\n",
      "max_leaf_nodes = 1000, train-single mae:  0.11507936507936507\n",
      "max_leaf_nodes = 1000, train-multi mae:  0.12337662337662338\n",
      "max_leaf_nodes = 5000, train-single mae:  0.11507936507936507\n",
      "max_leaf_nodes = 5000, train-multi mae:  0.12337662337662338\n"
     ]
    }
   ],
   "source": [
    "for leaves in (10, 50, 100, 500, 1000, 5000):\n",
    "    print(f\"max_leaf_nodes = {leaves}, train-single mae: \", try_leaves(leaves, single_X, single_y, multi_X, multi_y))\n",
    "    print(f\"max_leaf_nodes = {leaves}, train-multi mae: \", try_leaves(leaves, multi_X, multi_y, single_X, single_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_leaf_node=50 with singlemodal data as training data seems to be optimal setting for current configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 36 entries, 0 to 248\n",
      "Columns: 5001 entries, 1 to Vtype\n",
      "dtypes: float64(5001)\n",
      "memory usage: 1.4 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 36 entries, 0 to 248\n",
      "Columns: 5000 entries, 1 to 5000\n",
      "dtypes: float64(5000)\n",
      "memory usage: 1.4 MB\n",
      "None\n",
      "[0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "17\n",
      "47.22222222222222\n"
     ]
    }
   ],
   "source": [
    "fear_detect_model = DecisionTreeClassifier(max_leaf_nodes=50, random_state=1)\n",
    "\n",
    "fear_detect_model.fit(single_X, single_y)\n",
    "\n",
    "# try to test our model on fear sessions\n",
    "fear_multi_modal =  multi_modal_df.query(\"`Vtype` == 1\")\n",
    "print(fear_multi_modal.info())\n",
    "val_X = fear_multi_modal[list(range(1, 5001))]\n",
    "print(val_X.info())\n",
    "val_predictions = fear_detect_model.predict(val_X)\n",
    "print(val_predictions)\n",
    "# as all of the entries in val_prediction should ideally be equal to 7.0:\n",
    "counter = 0\n",
    "for each in val_predictions:\n",
    "    if round(each, 0) == 1.0:\n",
    "        counter += 1\n",
    "print(counter)\n",
    "# getting the % of correct answers:\n",
    "print(100* (counter / len(val_predictions)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy is 47%\n",
    "TT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00351687 -0.0001571  -0.01133531 ... -0.00811949 -0.00942752\n",
      " -0.00815506]\n",
      "[-0.00634845 -0.01122435 -0.01985871 ... -0.00122269 -0.00088196\n",
      "  0.00080364]\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import neurokit2 as nk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic signals\n",
    "# ecg = nk.ecg_simulate(duration=10, heart_rate=70)\n",
    "\n",
    "ecg_data = real_sm_dfs[0]\n",
    "\n",
    "# Load raw ECG data (replace with your data)\n",
    "# Assuming 'ecg_data' is a list or array of 5000 data points\n",
    "# ecg_data = [your_raw_ecg_data]  # Replace with actual data\n",
    "sampling_rate = 5000 / 39  # Approximate sampling rate\n",
    "\n",
    "# Step 1: Clean the ECG signal\n",
    "ecg_cleaned = nk.ecg_clean(ecg_data, sampling_rate=sampling_rate)\n",
    "print(ecg_cleaned)\n",
    "# Step 2: Process the ECG signal to find R-peaks\n",
    "signals, info = nk.ecg_process(ecg_cleaned, sampling_rate=sampling_rate)\n",
    "# nk.ecg_plot(signals, info)\n",
    "print(signals[\"ECG_Clean\"].values)\n",
    "\n",
    "# Step 3: Extract R-peaks, indexes where R-peaks are located\n",
    "r_peaks = info[\"ECG_R_Peaks\"] # np.ndarray\n",
    "tl = r_peaks.tolist()\n",
    "print(type(tl[0]))\n",
    "\n",
    "# Step 4: Delineate QRS complex\n",
    "delineate = nk.ecg_delineate(ecg_cleaned, r_peaks, sampling_rate=sampling_rate, method=\"peak\")\n",
    "\n",
    "# Access the Q, R, and S points\n",
    "q_points = delineate[0][\"ECG_Q_Peaks\"]\n",
    "s_points = delineate[0][\"ECG_S_Peaks\"]\n",
    "\n",
    "# Step 5: Plot the ECG with identified R-peaks, Q, and S points\n",
    "# plt.plot(ecg_cleaned, label=\"ECG Signal\")\n",
    "# plt.scatter(r_peaks, ecg_cleaned[r_peaks], color=\"red\", label=\"R-peaks\")\n",
    "# plt.scatter(q_points, ecg_cleaned[q_points], color=\"blue\", label=\"Q points\")\n",
    "# plt.scatter(s_points, ecg_cleaned[s_points], color=\"green\", label=\"S points\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# nk.hrv(r_peaks, sampling_rate=sampling_rate, show=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will repeat the previous approach, only just add the ecg_process method, and remove normalisation\n",
    "cleaned_single_modal_df = []\n",
    "columns = list(range(1, 5001)) + [\"Vtype\"]\n",
    "sampling_rate = 5000 / 39\n",
    "\n",
    "# We put vtype 1 for fear sessions, and vtype 0 for everything else\n",
    "for file in single_modal_data_files:\n",
    "    df = pd.read_csv(path_single_modal_data + file, delimiter='\\t')\n",
    "    vtype = float(file.split('v')[1][0])\n",
    "\n",
    "    vtype = 1 if vtype == 7.0 else 0\n",
    "\n",
    "    rdf = str_dfs_to_real_dfs([df])[0]\n",
    "    rdf_cleaned, info = nk.ecg_process(rdf, sampling_rate=sampling_rate)\n",
    "    rdf_and_vtype = pd.array([each for each in rdf_cleaned[\"ECG_Clean\"].values[info[\"ECG_R_Peaks\"]]] + [vtype])\n",
    "    cleaned_single_modal_df.append(rdf_and_vtype)\n",
    "cleaned_single_modal_df = pd.DataFrame(single_modal_df, columns=columns)\n",
    "\n",
    "\n",
    "cleaned_multi_modal_df = []\n",
    "for file in multi_modal_data_files:\n",
    "    df = pd.read_csv(path_multi_modal_data + file, delimiter='\\t')\n",
    "    vtype = float(file.split('v')[1][0])\n",
    "\n",
    "    vtype = 1 if vtype == 7.0 else 0\n",
    "\n",
    "    rdf = str_dfs_to_real_dfs([df])[0]\n",
    "    rdf_cleaned, info = nk.ecg_process(rdf, sampling_rate=sampling_rate)\n",
    "    rdf_and_vtype = pd.array([each for each in rdf_cleaned[\"ECG_Clean\"].values[info[\"ECG_R_Peaks\"]]] + [vtype])\n",
    "    cleaned_multi_modal_df.append(rdf_and_vtype)\n",
    "cleaned_multi_modal_df = pd.DataFrame(multi_modal_df, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_single_y = cleaned_single_modal_df.Vtype\n",
    "cleaned_single_X = cleaned_single_modal_df[list(range(1, 5001))]\n",
    "\n",
    "cleaned_multi_y = cleaned_multi_modal_df.Vtype\n",
    "cleaned_multi_X = cleaned_multi_modal_df[list(range(1, 5001))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11507936507936507\n"
     ]
    }
   ],
   "source": [
    "max_leaf_nodes = 50\n",
    "cleaned_fear_detect_model = DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes, random_state=1)\n",
    "\n",
    "cleaned_fear_detect_model.fit(cleaned_single_X, cleaned_single_y)\n",
    "\n",
    "cleaned_val_predictions = cleaned_fear_detect_model.predict(cleaned_multi_X)\n",
    "print(mean_absolute_error(cleaned_multi_y, cleaned_val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 36 entries, 0 to 248\n",
      "Columns: 5001 entries, 1 to Vtype\n",
      "dtypes: float64(5001)\n",
      "memory usage: 1.4 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 36 entries, 0 to 248\n",
      "Columns: 5000 entries, 1 to 5000\n",
      "dtypes: float64(5000)\n",
      "memory usage: 1.4 MB\n",
      "None\n",
      "[0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "17\n",
      "47.22222222222222\n"
     ]
    }
   ],
   "source": [
    "# try to test our model on fear sessions\n",
    "cleaned_fear_multi_modal =  cleaned_multi_modal_df.query(\"`Vtype` == 1\")\n",
    "print(cleaned_fear_multi_modal.info())\n",
    "cleaned_val_X = cleaned_fear_multi_modal[list(range(1, 5001))]\n",
    "print(cleaned_val_X.info())\n",
    "cleaned_val_predictions = cleaned_fear_detect_model.predict(cleaned_val_X)\n",
    "print(cleaned_val_predictions)\n",
    "# as all of the entries in val_prediction should ideally be equal to 1.0:\n",
    "counter = 0\n",
    "for each in cleaned_val_predictions:\n",
    "    if round(each, 0) == 1.0:\n",
    "        counter += 1\n",
    "print(counter)\n",
    "# getting the % of correct answers:\n",
    "print(100* (counter / len(cleaned_val_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyway, we got the same result. Now, lets try to give more data for the training, and less for predicting. Divide traing to test as 75:25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12745098039215685\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Combine the datasets\n",
    "X = pd.concat([cleaned_single_X, cleaned_multi_X], ignore_index=True)\n",
    "y = pd.concat([cleaned_single_y, cleaned_multi_y], ignore_index=True)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_X, test_val_X, train_y, test_val_y = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "\n",
    "# Initialize and train the model\n",
    "bigger_fear_detect_model = DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes, random_state=1)\n",
    "bigger_fear_detect_model.fit(train_X, train_y)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "bigger_val_predictions = bigger_fear_detect_model.predict(test_val_X)\n",
    "print(mean_absolute_error(test_val_y, bigger_val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 14\n",
      "57.14285714285714\n"
     ]
    }
   ],
   "source": [
    "# try to test our model on fear sessions\n",
    "test_fear =  pd.DataFrame(test_val_y).query(\"`Vtype` == 1\")\n",
    "fear_val_X = X[X.index.isin(test_fear.index.values)]\n",
    "fear_val_predictions = bigger_fear_detect_model.predict(fear_val_X)\n",
    "\n",
    "# as all of the entries in val_prediction should ideally be equal to 1.0:\n",
    "counter = 0\n",
    "for each in fear_val_predictions:\n",
    "    if round(each, 0) == 1.0:\n",
    "        counter += 1\n",
    "print(counter, len(fear_val_predictions))\n",
    "# getting the % of correct answers:\n",
    "print(100* (counter / len(fear_val_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is now 57%, and have been increased by ~10%. Great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
